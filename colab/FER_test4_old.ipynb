{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfW-hFwijFae",
        "outputId": "32b96af7-2905-420f-c3fe-db983f39f091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "keras_vggface is already installed with version 0.6\n",
            "keras_applications is already installed with version 1.0.8\n",
            "batch_face is already installed with version 1.4.0\n",
            "Directory already exists. No need to extract the ZIP file.\n",
            "Google Drive is already mounted.\n",
            "drive  face_areas.pkl  notebook_modules  notebook_modules.zip  sample_data\n",
            "['RetinaFace', 'VideoCamera', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'cv2', 'img_to_array', 'np', 'os', 'utils']\n",
            "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'get_face_areas', 'load_weights_EE', 'load_weights_LSTM', 'np', 'pickle', 'pred_one_video', 'sequences', 'stats', 'time', 'warnings']\n",
            "Name video:  1001_DFA_ANG_XX.mp4\n",
            "Number total of frames:  68\n",
            "FPS:  30.0\n",
            "Video duration: 2.27 s\n",
            "Frame width: 480\n",
            "Frame height: 360\n",
            "Number of frames after sampling:  12\n",
            "Backbone model:  /content/drive/MyDrive/Thesis_Data/CREMA_runs/Pretrained_models/Backbone_models/weights_0_66_37_wo_gl.h5\n",
            "LSTM model:  /content/drive/MyDrive/Thesis_Data/CREMA_runs/Pretrained_models/LSTM_models/CREMA-D_with_config.h5\n",
            "Shape of features:  (12, 512)\n",
            "Step size:  4\n",
            "Window size:  5\n",
            "Sampled frames  [0, 4, 8, 12]\n",
            "Number of steps/windows:  3\n",
            "Shape of pred:  (3, 7)\n",
            "Shape of argmax:  (3,)\n",
            "argmax:  [0 0 0]\n",
            "Predicted emotion:  Neutral\n",
            "True emotion:  Anger\n",
            "Lead time: 6.49 s\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [filename, emotion, predicted, argmax]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "import pkg_resources\n",
        "# Install packages\n",
        "package_names = ['keras_vggface', 'keras_applications', 'batch_face']\n",
        "for package_name in package_names:\n",
        "  try:\n",
        "    dist = pkg_resources.get_distribution(package_name)\n",
        "    print(f\"{package_name} is already installed with version {dist.version}\")\n",
        "  except pkg_resources.DistributionNotFound:\n",
        "    print(f\"{package_name} is not installed, installing now...\")\n",
        "    !pip install {package_name}\n",
        "\n",
        "# !pip install --upgrade --force-reinstall keras_vggface\n",
        "# !pip install keras_vggface\n",
        "# !pip install keras_applications\n",
        "# !pip install batch_face\n",
        "\n",
        "# Fix package files\n",
        "!sed -i 's/from keras.utils import layer_utils/from tensorflow.python.keras.utils import layer_utils/' /usr/local/lib/python3.10/dist-packages/keras_vggface/models.py\n",
        "!sed -i 's/from keras.utils.data_utils import get_file/from tensorflow.python.keras.utils.data_utils import get_file/' /usr/local/lib/python3.10/dist-packages/keras_vggface/models.py\n",
        "!sed -i 's/from keras.utils.data_utils import get_file/from tensorflow.python.keras.utils.data_utils import get_file/' /usr/local/lib/python3.10/dist-packages/keras_vggface/utils.py\n",
        "!sed -i 's/from keras.engine.topology import get_source_inputs/from tensorflow.python.keras.utils.layer_utils import get_source_inputs/' /usr/local/lib/python3.10/dist-packages/keras_vggface/models.py\n",
        "\n",
        "# import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from scipy import stats\n",
        "# import pickle\n",
        "import sys\n",
        "\n",
        "expected_directory = '/content/notebook_modules'\n",
        "\n",
        "# Check if the expected directory or file exists\n",
        "if not os.path.exists(expected_directory):\n",
        "    print(\"Modules directory does not exist, extracting the ZIP file...\")\n",
        "    # Extract the ZIP file since the directory doesn't exist\n",
        "    !unzip '/content/notebook_modules.zip' -d /content/\n",
        "else:\n",
        "    print(\"Modules directory already exists. No need to extract the ZIP file.\")\n",
        "\n",
        "sys.path.append('/content/notebook_modules')\n",
        "\n",
        "import sequences\n",
        "import get_face_areas\n",
        "from get_models import load_weights_EE, load_weights_LSTM\n",
        "import run_functions\n",
        "\n",
        "# from select_video_subset import select_video_subset\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category = FutureWarning)\n",
        "\n",
        "# Mount drive\n",
        "from google.colab import drive\n",
        "\n",
        "def is_drive_mounted():\n",
        "    drive_path = '/content/drive'\n",
        "    return os.path.isdir(drive_path) and os.listdir(drive_path)\n",
        "\n",
        "# Mount Google Drive if it is not already mounted\n",
        "if not is_drive_mounted():\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive is mounted now!\")\n",
        "else:\n",
        "    print(\"Google Drive is already mounted.\")\n",
        "\n",
        "# Test runs are saved in Google Drive in folders named Run_1, Run_2, etc.\n",
        "# Check the number of the latest run folder\n",
        "run_folders = [folder for folder in os.listdir('/content/drive/MyDrive/Thesis_Data/CREMA_runs') if 'Run_' in folder]\n",
        "\n",
        "continue_from_checkpoint = False\n",
        "\n",
        "# If run folders is empty, this is the first run\n",
        "# Check if run folders is empty\n",
        "if not run_folders:\n",
        "    this_run = 1\n",
        "    this_run_folder = f'/content/drive/MyDrive/Thesis_Data/CREMA_runs/Run_{this_run}'\n",
        "    os.makedirs(this_run_folder)\n",
        "    # Create subfolder for checkpoints\n",
        "    checkpoint_folder = os.path.join(this_run_folder, 'checkpoints')\n",
        "    os.makedirs(checkpoint_folder)\n",
        "elif continue_from_checkpoint:\n",
        "    latest_run = max([int(folder.split('_')[1]) for folder in run_folders])\n",
        "    this_run = latest_run\n",
        "    checkpoint_files = [csv_file for csv_file in os.listdir(f'/content/drive/MyDrive/Thesis_Data/CREMA_runs/Run_{this_run}/checkpoints')]\n",
        "    last_checkpoint = max([int(checkpoint_file.split(\"_\")[-1].split(\".\")[0]) for checkpoint_file in checkpoint_files])\n",
        "    print(f\"Continuing from checkpoint {last_checkpoint}.\")\n",
        "else:\n",
        "    latest_run = max([int(folder.split('_')[1]) for folder in run_folders])\n",
        "    this_run = latest_run + 1\n",
        "    this_run_folder = f'/content/drive/MyDrive/Thesis_Data/CREMA_runs/Run_{this_run}'\n",
        "    os.makedirs(this_run_folder)\n",
        "    # Create subfolder for checkpoints\n",
        "    checkpoint_folder = os.path.join(this_run_folder, 'checkpoints')\n",
        "    os.makedirs(checkpoint_folder)\n",
        "\n",
        "# Data folder\n",
        "data_folder = '/content/VideoMP4'\n",
        "\n",
        "# Extract zip folder on Google Drive and save it in Google Colab workspace\n",
        "if not os.path.exists(data_folder):\n",
        "    print(\"Data folder does not exist, extracting the ZIP file...\")\n",
        "    # Extract the ZIP file since the directory doesn't exist\n",
        "    !unzip '/content/drive/MyDrive/Thesis_Data/CREMA_runs/video_files.zip' -d /content/\n",
        "else:\n",
        "    print(\"Data folder already exists. No need to extract the ZIP file.\")\n",
        "\n",
        "backbone_model_path = '/content/drive/MyDrive/Thesis_Data/CREMA_runs/Pretrained_models/Backbone_models/weights_0_66_37_wo_gl.h5'\n",
        "LSTM_model_path = '/content/drive/MyDrive/Thesis_Data/CREMA_runs/Pretrained_models/LSTM_models/CREMA-D_with_config.h5'\n",
        "\n",
        "emotion_dict = {\"NEU\": \"Neutral\", \"HAP\": \"Happiness\", \"SAD\": \"Sadness\", \"SUR\": \"Surprise\", \"FEA\": \"Fear\", \"DIS\": \"Disgust\", \"ANG\": \"Anger\"}\n",
        "\n",
        "# Create a list of all video files in the data folder if file ends with .mp4\n",
        "video_files = [file for file in os.listdir(data_folder) if file.endswith(\".mp4\")]\n",
        "# print(\"video_files: \", video_files)\n",
        "# print()\n",
        "\n",
        "data = pd.DataFrame(columns=['filename', 'emotion', 'predicted', 'argmax', 'checkpoint'])\n",
        "data['filename'] = video_files\n",
        "data['emotion'] = [emotion_dict[video_file.split(\"_\")[2]] for video_file in video_files]\n",
        "\n",
        "if continue_from_checkpoint:\n",
        "    video_files = video_files[last_checkpoint:]\n",
        "    progress = last_checkpoint + 1\n",
        "else:\n",
        "    progress = 1\n",
        "\n",
        "checkpoint_row = 1\n",
        "for video_file in video_files:\n",
        "    print(f\"Processing video {progress}/{len(video_files)}\")\n",
        "    # Get emotion from substring in file name\n",
        "    true_emotion = emotion_dict[video_file.split(\"_\")[2]]\n",
        "    predicted_emotion, predicted_argmax = run_functions.pred_one_video(data_folder, video_file, backbone_model_path, LSTM_model_path)\n",
        "    # predicted_emotion, predicted_argmax = run_functions.pred_one_video(test_video_path, test_true_label, backbone_model_path, LSTM_model_path)\n",
        "    data.loc[data['filename'] == video_file, 'predicted'] = predicted_emotion\n",
        "    data.loc[data['filename'] == video_file, 'argmax'] = predicted_argmax\n",
        "    data.loc[data['filename'] == video_file, 'checkpoint'] = progress\n",
        "    if progress % 100 == 0:\n",
        "        # save the DataFrame to a csv file\n",
        "        checkpoint_data = data.head(checkpoint_row)\n",
        "        checkpoint_data.to_csv(os.path.join(checkpoint_folder,f'run_{this_run}_predicted_checkpoint_{progress}.csv'), index=False)\n",
        "        print(f\"Checkpoint {progress} saved.\")\n",
        "    progress += 1\n",
        "    checkpoint_row += 1\n",
        "\n",
        "# save the DataFrame to a csv file\n",
        "data.to_csv(os.path.join(this_run_folder,f'run_{this_run}_predicted.csv'), index=False)\n",
        "data.head()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
